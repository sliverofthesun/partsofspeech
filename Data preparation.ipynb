{"cells":[{"cell_type":"markdown","source":["# Data preparation"],"metadata":{"id":"1I6EI9IHHCgi"}},{"cell_type":"markdown","source":["The following notebook prepares the data for a parts-of-speech tagging model using the croatian language."],"metadata":{"id":"mopFSelMHTem"}},{"cell_type":"code","source":["!git clone https://github.com/facebookresearch/fastText.git\n","!pip install fastText/."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5DxI_LOH0iN","executionInfo":{"status":"ok","timestamp":1708447951793,"user_tz":-60,"elapsed":84750,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"}},"outputId":"d63ceabd-8dc3-45c4-b6b3-6d6aa53aa01a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'fastText'...\n","remote: Enumerating objects: 3995, done.\u001b[K\n","remote: Counting objects: 100% (1023/1023), done.\u001b[K\n","remote: Compressing objects: 100% (183/183), done.\u001b[K\n","remote: Total 3995 (delta 893), reused 863 (delta 835), pack-reused 2972\u001b[K\n","Receiving objects: 100% (3995/3995), 8.29 MiB | 15.24 MiB/s, done.\n","Resolving deltas: 100% (2531/2531), done.\n","Processing ./fastText\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pybind11>=2.2 (from fasttext==0.9.2)\n","  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.2) (67.7.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.2) (1.25.2)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4214525 sha256=89e5016d7393b5852b2b25f8fbc8e1cdb41487e4097f2dc34cb98eb9df499172\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-00fw36qo/wheels/8b/05/af/3cfae069d904597d44b309c956601b611bdf8967bcbe968903\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"]}]},{"cell_type":"markdown","source":["Import of needed libraries:"],"metadata":{"id":"gIprSPTHHfWs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_1fG-2y4W91"},"outputs":[],"source":["import fasttext\n","import fasttext.util\n","import gzip\n","import glob\n","import numpy as np\n","import os\n","import pandas as pd\n","import pickle\n","import shutil\n","from sklearn.model_selection import train_test_split\n","import string\n","from google.colab import drive\n","import xml.etree.ElementTree as ET"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90718,"status":"ok","timestamp":1708448043794,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"},"user_tz":-60},"id":"QYPwd6uo173C","outputId":"592b0d1c-7782-46aa-9456-13e9cf08037d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"Xwx30lLJ3GTF"},"source":["###Data Download"]},{"cell_type":"markdown","source":["The data is found at https://www.clarin.si/repository/xmlui/handle/11356/1064 and contains 14 compressed xml files.\n","\n","Due to the immense difference between the available compute and the size of the dataset, we will use just 0.14% of the full dataset.\n","\n","If the full dataset were to be used, as you will see in more detail in the \"training model\" notebook, the training would take ~424 hours."],"metadata":{"id":"0DfcnsF9HHF7"}},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/opj/data\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6pnyaZ3MY0L","executionInfo":{"status":"ok","timestamp":1708444182413,"user_tz":-60,"elapsed":6,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"}},"outputId":"71377e74-d5c2-4f9a-e64b-f87368487fd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/opj/data\n","checkpoints  fastText  hrWaC2.1.01_1.txt  hrWaC2.1.01_1.xml  hrWaC2.1.01.xml.gz  train\tvalidation\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":713847,"status":"ok","timestamp":1705418726865,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"},"user_tz":-60},"id":"-vOyduqr2RtS","outputId":"5c49b902-765a-44ac-b582-3372575926fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  618M  100  618M    0     0   887k      0  0:11:53  0:11:53 --:--:--  249k\n"]}],"source":["!curl --remote-name-all https://www.clarin.si/repository/xmlui/bitstream/handle/11356/1064{/hrWaC2.1.01.xml.gz}"]},{"cell_type":"markdown","metadata":{"id":"9MtdGWTU4xqA"},"source":["###Data Extraction"]},{"cell_type":"markdown","source":["Lets unzip the file and split it into 100 000 sentence long chunks."],"metadata":{"id":"MSwAAVU0JG3J"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcATefu_iIqw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708445020944,"user_tz":-60,"elapsed":229439,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"}},"outputId":"7367abb7-453c-44ec-aeeb-18dd3e182c29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gunzipping file: /content/gdrive/MyDrive/opj/data/hrWaC2.1.01.xml.gz\n","Splitting XML into chunks: /content/gdrive/MyDrive/opj/data/hrWaC2.1.01.xml\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Processed 25.0% of sentences\n","Processed 50.0% of sentences\n","Processed 75.0% of sentences\n","Processed 100.0% of sentences\n","Original XML file removed: /content/gdrive/MyDrive/opj/data/hrWaC2.1.01.xml\n"]}],"source":["def gunzip_file(gz_file_path, xml_file_path):\n","    print(f\"Gunzipping file: {gz_file_path}\")\n","    with gzip.open(gz_file_path, 'rb') as f_in:\n","        with open(xml_file_path, 'wb') as f_out:\n","            shutil.copyfileobj(f_in, f_out)\n","\n","def split_xml_into_chunks(original_xml_path, chunk_size):\n","    print(f\"Splitting XML into chunks: {original_xml_path}\")\n","    base_name, extension = os.path.splitext(os.path.basename(original_xml_path))\n","    current_chunk = 1\n","    sentences_in_current_chunk = 0\n","\n","    with open(original_xml_path, 'r', encoding='utf-8') as original_file:\n","        with open(f'{base_name}_{current_chunk}{extension}', 'w', encoding='utf-8') as chunk_file:\n","            for line in original_file:\n","                chunk_file.write(line)\n","\n","                if '</s>' in line:\n","                    sentences_in_current_chunk += 1\n","                    if sentences_in_current_chunk % int(chunk_size/4) == 0:\n","                        print(f\"Processed {(sentences_in_current_chunk/chunk_size)*100}% of sentences\")\n","\n","                if sentences_in_current_chunk >= chunk_size:\n","                    current_chunk += 1\n","                    sentences_in_current_chunk = 0\n","                    chunk_file.close()\n","                    chunk_file = open(f'{base_name}_{current_chunk}{extension}', 'w', encoding='utf-8')\n","\n","    open(original_xml_path, 'w').close()\n","    os.remove(original_xml_path)\n","    print(f\"Original XML file removed: {original_xml_path}\")\n","\n","xml_gz_file_path = '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01.xml.gz'\n","xml_file_path = '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01.xml'\n","gunzip_file(xml_gz_file_path, xml_file_path)\n","split_xml_into_chunks(xml_file_path, chunk_size=100000)"]},{"cell_type":"markdown","source":["For ease of data extraction, we convert all the xml files into txt:"],"metadata":{"id":"mt_Sk2tdJq9i"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLJ9f0KNNkvT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708445021665,"user_tz":-60,"elapsed":731,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"}},"outputId":"82adb41b-c473-420f-d505-fd9c96a65d91"},"outputs":[{"output_type":"stream","name":"stdout","text":["Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_1.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_1.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_2.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_2.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_3.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_3.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_4.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_4.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_5.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_5.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_6.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_6.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_7.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_7.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_8.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_8.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_9.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_9.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_10.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_10.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_11.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_11.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_12.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_12.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_13.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_13.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_14.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_14.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_15.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_15.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_16.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_16.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_17.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_17.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_18.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_18.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_19.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_19.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_20.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_20.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_21.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_21.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_22.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_22.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_23.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_23.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_24.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_24.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_25.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_25.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_26.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_26.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_27.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_27.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_28.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_28.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_29.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_29.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_30.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_30.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_31.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_31.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_32.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_32.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_33.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_33.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_34.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_34.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_35.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_35.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_36.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_36.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_37.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_37.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_38.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_38.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_39.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_39.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_40.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_40.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_41.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_41.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_42.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_42.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_43.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_43.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_44.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_44.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_45.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_45.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_46.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_46.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_47.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_47.txt'\n","Renamed '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_48.xml' to '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_48.txt'\n"]}],"source":["directory = '/content/gdrive/MyDrive/opj/data/'\n","pattern = os.path.join(directory, '*.xml')\n","\n","xml_files = glob.glob(pattern)\n","\n","for xml_file in xml_files:\n","    txt_file = xml_file[:-4] + '.txt'  #replaces the extension\n","    os.rename(xml_file, txt_file)\n","    print(f\"Renamed '{xml_file}' to '{txt_file}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2p0_qFSOtMJ"},"outputs":[],"source":["def load_and_process_data(filepath):\n","    sentences = []  # List to hold sentences, lists of (word, tag) tuples\n","    current_sentence = []\n","\n","    with open(filepath, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            line = line.strip()\n","\n","            # Starting a new sentence\n","            if line == \"<s>\":\n","                current_sentence = []\n","            # End the current sentence\n","            elif line == \"</s>\":\n","                if current_sentence:\n","                    sentences.append(current_sentence)\n","                    current_sentence = []\n","            # Process lines that are not xml tags or sentence markers\n","            elif not line.startswith(\"<\"):\n","                parts = line.split(\"\t\")\n","                if len(parts) >= 4:\n","                  word = parts[0]\n","                  tag = parts[3]  # Tag is the fourth element\n","                  current_sentence.append((word, tag))\n","\n","    return sentences\n","\n","filepath = '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_1.txt'\n","sentences = load_and_process_data(filepath)"]},{"cell_type":"markdown","source":["Lets take a look at one of the sentences;"],"metadata":{"id":"NZN0qET1QHs7"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708445325066,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"},"user_tz":-60},"id":"cVImhHPiOvk9","outputId":"b2bb6447-8b6c-4822-d6ff-a344e9e02b51"},"outputs":[{"output_type":"stream","name":"stdout","text":["First sentence: Adam se otpočetka nudi s tri benzinska motora : 1.2 70 KS , 1.4 87 KS i 1.4 100 KS .\n","With tags: [('Adam', 'Npmsn'), ('se', 'Px--sa'), ('otpočetka', 'Rgp'), ('nudi', 'Vmr3s'), ('s', 'Si'), ('tri', 'Mlc'), ('benzinska', 'Agpmsgn'), ('motora', 'Ncmsg'), (':', 'Z'), ('1.2', 'Mdc'), ('70', 'Mdc'), ('KS', 'Y'), (',', 'Z'), ('1.4', 'Mdc'), ('87', 'Mdc'), ('KS', 'Y'), ('i', 'Cc'), ('1.4', 'Mdc'), ('100', 'Mdc'), ('KS', 'Y'), ('.', 'Z')]\n"]}],"source":["if sentences:\n","    first_sentence = sentences[0]\n","    sentence_str = ' '.join([word for word, tag in first_sentence])\n","    print(\"First sentence:\", sentence_str)\n","    print(\"With tags:\", first_sentence)\n","else:\n","    print(\"No sentences found in the data.\")"]},{"cell_type":"markdown","source":["The next step is to embed the words into a numerical representation usable by the model.\n","\n","We will be using a preexisting embedding found at https://fasttext.cc/docs/en/crawl-vectors.html"],"metadata":{"id":"lWspULe9RxVc"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110303,"status":"ok","timestamp":1708448154090,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"},"user_tz":-60},"id":"CyciB9tzRi0i","outputId":"fb502fbf-eaee-46a8-c92e-56459a1e56c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 4299M  100 4299M    0     0  38.1M      0  0:01:52  0:01:52 --:--:-- 38.5M\n"]}],"source":["!curl https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hr.300.bin.gz --output cc.hr.300.bin.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":128016,"status":"ok","timestamp":1708448282103,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"},"user_tz":-60},"id":"06rTDLbYSZXN","outputId":"012762b1-db64-4208-f039-ba9308c6f347"},"outputs":[{"output_type":"stream","name":"stdout","text":["gzip: cc.hr.300.bin: unknown suffix -- ignored\n"]}],"source":["!gunzip cc.hr.300.bin.gz -d cc.hr.300.bin"]},{"cell_type":"markdown","source":["Lets load the embedding and double check the dimensions"],"metadata":{"id":"NnoEVeVCSW-g"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708448282428,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"},"user_tz":-60},"id":"8-pk0UixQqSo","outputId":"205dcb0b-c224-49f5-ceac-7ba3b1824aa1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{},"execution_count":6}],"source":["ft = fasttext.load_model('cc.hr.300.bin')\n","ft.get_dimension()"]},{"cell_type":"markdown","source":["Originally, I planned to reduce the embeddings to 100 dimensions, but resource constrains and fair performance on 300-dim vector, made me reconsider."],"metadata":{"id":"Tz3lmbt0SciO"}},{"cell_type":"code","source":["#fasttext.util.reduce_model(ft, 100)\n","#ft.get_dimension()"],"metadata":{"id":"p3G1geHTwaB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testing out the embeddings:"],"metadata":{"id":"wZSZ3egwSqKb"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27871,"status":"ok","timestamp":1708446033929,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"},"user_tz":-60},"id":"r5nhmoq6U0F_","outputId":"0604599b-7e26-45ce-ed9a-cbb476ce8c1a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0.687420666217804, 'zagreb.'),\n"," (0.6478269100189209, 'zagrebčki'),\n"," (0.6400067806243896, 'zagrebu'),\n"," (0.637527585029602, 'zagreb.hr'),\n"," (0.6205328106880188, '.zagreb'),\n"," (0.6144486665725708, 'zagrebli'),\n"," (0.6062915921211243, 'zagrebdox.net'),\n"," (0.6061169505119324, 'zagrebemo'),\n"," (0.6033006906509399, 'zagrebu.'),\n"," (0.5954848527908325, 'zagre-')]"]},"metadata":{},"execution_count":32}],"source":["ft.get_nearest_neighbors('zagreb')"]},{"cell_type":"markdown","metadata":{"id":"RhuQN845V-oq"},"source":["Lets turn the words into embeddings and the tags into one of 12 possible classes. One for each type of part of speech."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-B_qTR0V-Wv"},"outputs":[],"source":["def is_punctuation(word):\n","    return all(char in string.punctuation for char in word)\n","\n","def tag_to_integer(tag):\n","    tag_to_int = {\n","        'N': 0,  # Noun\n","        'V': 1,  # Verb\n","        'A': 2,  # Adjective\n","        'P': 3,  # Pronoun\n","        'R': 4,  # Adverb\n","        'S': 5,  # Adposition\n","        'C': 6,  # Conjunction\n","        'M': 7,  # Numeral\n","        'Q': 8,  # Particle\n","        'I': 9,  # Interjection\n","        'Y': 10, # Abbreviation\n","        'X': 11 # Residual\n","    }\n","\n","    return tag_to_int.get(tag, -1)\n","\n","def load_and_process_data_with_embeddings(filepath, ft_model):\n","    sentences_with_embeddings = []  # List to hold sentences with embeddings\n","\n","    with open(filepath, 'r', encoding='utf-8') as file:\n","        current_sentence = []  # Current sentence being processed\n","        for line in file:\n","            line = line.strip()\n","\n","            if line == \"<s>\":\n","                current_sentence = []\n","            elif line == \"</s>\":\n","                if current_sentence:\n","                    sentences_with_embeddings.append(current_sentence)\n","                    current_sentence = []\n","            elif not line.startswith(\"<\"):\n","                parts = line.split(\"\\t\")  # Ensure this matches the delimiter used in your file\n","                if len(parts) >= 4:\n","                    word = parts[0]\n","                    tag = parts[3]  # Assuming the tag is the fourth element\n","                    # Ignore punctuation\n","                    if not is_punctuation(word):\n","                        # Convert the word to its embedding\n","                        embedding = ft_model.get_word_vector(word)\n","                        current_sentence.append((embedding, tag_to_integer(tag[0])))\n","\n","    return sentences_with_embeddings\n","\n","filepath = '/content/gdrive/MyDrive/opj/data/hrWaC2.1.01_1.txt'\n","sentences_with_embeddings = load_and_process_data_with_embeddings(filepath, ft)"]},{"cell_type":"markdown","source":["We must also split the data into chunks so that we can load each during training."],"metadata":{"id":"2rWSEN8BU0_r"}},{"cell_type":"code","source":["def save_in_chunks(data, base_filepath, chunk_size=10000):\n","    for i in range(0, len(data), chunk_size):\n","        chunk = data[i:i+chunk_size]\n","        filepath = f\"{base_filepath}_part_{i//chunk_size}.pkl\"\n","        with open(filepath, 'wb') as file:\n","            pickle.dump(chunk, file)\n","        print(f\"Saved chunk {i//chunk_size} to {filepath}\")\n","\n","base_filepath = '/content/gdrive/MyDrive/opj/data/embedded_chunked'\n","save_in_chunks(sentences_with_embeddings, base_filepath)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lGAaTyn80qA5","executionInfo":{"status":"ok","timestamp":1708448545025,"user_tz":-60,"elapsed":145717,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"}},"outputId":"c29e58f3-398a-4bc8-d289-5bd81d234155"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved chunk 0 to /content/gdrive/MyDrive/opj/data/embedded_chunked_part_0.pkl\n","Saved chunk 1 to /content/gdrive/MyDrive/opj/data/embedded_chunked_part_1.pkl\n","Saved chunk 2 to /content/gdrive/MyDrive/opj/data/embedded_chunked_part_2.pkl\n","Saved chunk 3 to /content/gdrive/MyDrive/opj/data/embedded_chunked_part_3.pkl\n","Saved chunk 4 to /content/gdrive/MyDrive/opj/data/embedded_chunked_part_4.pkl\n","Saved chunk 5 to /content/gdrive/MyDrive/opj/data/embedded_chunked_part_5.pkl\n","Saved chunk 6 to /content/gdrive/MyDrive/opj/data/embedded_chunked_part_6.pkl\n","Saved chunk 7 to /content/gdrive/MyDrive/opj/data/embedded_chunked_part_7.pkl\n","Saved chunk 8 to /content/gdrive/MyDrive/opj/data/embedded_chunked_part_8.pkl\n","Saved chunk 9 to /content/gdrive/MyDrive/opj/data/embedded_chunked_part_9.pkl\n"]}]},{"cell_type":"markdown","source":["We will now pad the data to a len of 30, and split it into training and test datasets."],"metadata":{"id":"i4qu_yhdee3-"}},{"cell_type":"code","source":["def pad_embeddings(sequences, max_len, dim=300):\n","    padded = np.zeros((len(sequences), max_len, dim))\n","    for i, sequence in enumerate(sequences):\n","        length = min(len(sequence), max_len)\n","        padded[i, :length, :] = sequence[:length]\n","    return padded\n","\n","def pad_tags(sequences, max_len, pad_value=-1):\n","    padded = np.full((len(sequences), max_len), pad_value)\n","    for i, sequence in enumerate(sequences):\n","        length = min(len(sequence), max_len)\n","        padded[i, :length] = sequence[:length]\n","    return padded\n","\n","def process_and_save_file(filepath, max_len, train_path, val_path, index):\n","    with open(filepath, 'rb') as file:\n","        sentences_with_embeddings = pickle.load(file)\n","\n","        embeddings = [[emb for emb, _ in sentence] for sentence in sentences_with_embeddings]\n","        tags = [[tag for _, tag in sentence] for sentence in sentences_with_embeddings]\n","\n","        # Padding\n","        embeddings_padded = pad_embeddings(embeddings, max_len)\n","        tags_padded = pad_tags(tags, max_len)\n","\n","        X_train, X_val, y_train, y_val = train_test_split(embeddings_padded, tags_padded, test_size=0.1, random_state=42)\n","\n","        train_filepath = os.path.join(train_path, f'train_{index}.pkl')\n","        val_filepath = os.path.join(val_path, f'validation_{index}.pkl')\n","\n","        with open(train_filepath, 'wb') as f:\n","            pickle.dump((X_train, y_train), f)\n","\n","        with open(val_filepath, 'wb') as f:\n","            pickle.dump((X_val, y_val), f)\n","\n","        print(f\"Processed and saved: {train_filepath} and {val_filepath}\")\n","\n","\n","max_len = 30\n","directory = '/content/gdrive/MyDrive/opj/data/'\n","train_path = '/content/gdrive/MyDrive/opj/data/train/'\n","val_path = '/content/gdrive/MyDrive/opj/data/validation/'\n","\n","os.makedirs(train_path, exist_ok=True)\n","os.makedirs(val_path, exist_ok=True)\n","\n","filepaths = glob.glob(os.path.join(directory, '*.pkl'))\n","for i, filepath in enumerate(sorted(filepaths), start=1):\n","    process_and_save_file(filepath, max_len, train_path, val_path, i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCuRcU6Z4yLr","executionInfo":{"status":"ok","timestamp":1708435306658,"user_tz":-60,"elapsed":228961,"user":{"displayName":"r4wtgrh","userId":"14922680703818809666"}},"outputId":"cb20031b-1ea8-4cd2-dd69-b7ec8bba4b01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed and saved: /content/gdrive/MyDrive/opj/data/train/train_1.pkl and /content/gdrive/MyDrive/opj/data/validation/validation_1.pkl\n","Processed and saved: /content/gdrive/MyDrive/opj/data/train/train_2.pkl and /content/gdrive/MyDrive/opj/data/validation/validation_2.pkl\n","Processed and saved: /content/gdrive/MyDrive/opj/data/train/train_3.pkl and /content/gdrive/MyDrive/opj/data/validation/validation_3.pkl\n","Processed and saved: /content/gdrive/MyDrive/opj/data/train/train_4.pkl and /content/gdrive/MyDrive/opj/data/validation/validation_4.pkl\n","Processed and saved: /content/gdrive/MyDrive/opj/data/train/train_5.pkl and /content/gdrive/MyDrive/opj/data/validation/validation_5.pkl\n","Processed and saved: /content/gdrive/MyDrive/opj/data/train/train_6.pkl and /content/gdrive/MyDrive/opj/data/validation/validation_6.pkl\n","Processed and saved: /content/gdrive/MyDrive/opj/data/train/train_7.pkl and /content/gdrive/MyDrive/opj/data/validation/validation_7.pkl\n","Processed and saved: /content/gdrive/MyDrive/opj/data/train/train_8.pkl and /content/gdrive/MyDrive/opj/data/validation/validation_8.pkl\n","Processed and saved: /content/gdrive/MyDrive/opj/data/train/train_9.pkl and /content/gdrive/MyDrive/opj/data/validation/validation_9.pkl\n","Processed and saved: /content/gdrive/MyDrive/opj/data/train/train_10.pkl and /content/gdrive/MyDrive/opj/data/validation/validation_10.pkl\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNac2lamULdHkGdrAk5QlyX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}